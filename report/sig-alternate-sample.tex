% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}


\begin{document}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{ML Exercise 2 - Group 19}
\subtitle{Classification Tasks}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Lukas Stanek \\
       \affaddr{Technical University of Vienna}\\
       \affaddr{Karlsplatz 13}\\
       \affaddr{1040, Vienna}\\
       \email{----}
% 2nd. author
Thomas Appler \\
       \affaddr{Technical University of Vienna}\\
       \affaddr{Karlsplatz 13}\\
       \affaddr{1040, Vienna}\\
       \email{----}
% 3rd. author
\alignauthor
Marten Sigwart \\
       \affaddr{Technical University of Vienna}\\
       \affaddr{Karlsplatz 13}\\
       \affaddr{1040, Vienna}\\
       \email{e1638152@student.tuwien.ac.at}
}
% There's nothing stop
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle


\section{Introduction}
Our task was to apply 4 different classification algorithms to 4 different data sets. Hereby, we had to experiment with different parameter settings of the classification algorithm, evaluate the performance by choosing appropriate performance measures and compare results among classifiers and data sets. We also had to perform some pre-processing steps on the data prior to applying the classifiers and study the impact of those pre-processing methods.

\subsection{4 data sets}
We chose four different data sets for the task. We chose data sets which varied in size, varied in the number of attributes and varied in the number of instances. Further we tried to pick data sets with different numbers of classes. Some data sets had missing values to take into account.

The four data sets we chose are:

\begin{itemize}
\item \textbf{Letter Recognition}
This data set contained attributes about letters shown on a rectangular, black and white display. The classification task is to determine the right letter (A-Z)
\item \textbf{Internet Advertisements}
This data set contained information about images of different web pages. The classification task is to determine whether or not an image is an advertisement.
\item \textbf{Leukemia}
This data set contained medical information of cancer patients. The classification task is to determine whether or not a certain patients has cancer.
\item \textbf{Congressional Voting}
This data set contains information on different voters. The classification task is to determine the party affiliation of the voters.
\end{itemize}

Each data set will be described in more detail in sections below.


\subsection{4 classification algorithms}
We chose 4 different classifiers for the task. Hereby, it was important that at least 3 of the classifiers derived from totally different learning algorithms. The classifiers we chose are:

\begin{itemize}
\item \textbf{K-Nearest Neighbors}
\item \textbf{Random Forrest}
\item \textbf{Decision Tree}
\item \textbf{Bayes Network}
\end{itemize}


\subsubsection{Algorithm 1: K-Nearest Neighbors}
KNN itself doesn't deal with missing values, it's the distance 
function. E.g., in case of euclidean distance, the following applies 
to missing values while computing the distance of the same attribute 
between two weka.core.Instance objects: 
- attributes in both Instance objects have missing value 
  => 0 
- only one value is missing 
  ~ nominal attribute 
    => 1 
  ~ numeric attribute 
    => 1 in case of normalization, maxRange-minRange for that 
attribute otherwise 
TODO\\
\subsubsection{Random Forest: Random Forrest}
TODO\\
\subsubsection{Decision Tree - 48J: Decision Tree}
TODO\\
\subsubsection{Bayes Network: Bayes??}
TODO\\

\subsection{Performance Measures}
TODO\\

% Section Data Set 1
\section{Data Set 1: Letter Recognition}
The objective for this data set is to predict a letter shown on a rectangular, black and white, display. There are 16 numerical features  provided for determining the correct letter. These attributes provide information about statistical properties of the letter like total number of pixels, width, mean of x-axis pixels, etc.
All of these values were then scaled into a range from 0 through 15. The 26 capital letters to predict in this set were taken from 20 different fonts. The letters are more or less evenly spread over the 20.000 instances, within a range of 734 to 813 occurrences. This data set contains no missing values. For comparable results 10-fold cross validation was used to measure the algorithms precision.
As for performance criteria with this data set only the percentage of correctly classified instances will be considered. The reason for this being that if a letter is wrongly classified, it doesn't matter which other letter was recognized instead of the correct one. 
\\
\subsection{Preprocessing}
As the data set was only provided as CSV file without header row, we added a header row for displaying the attribute names in Weka and therefore facilitating the analysis of the results. 
\\
\subsection{K-Nearest Neighbors}
In order to be able to compare the results we always used the same number of neighbors: 1,2,3,4,5,6,8,10,16,20,40,100.\\
The algorithm is generally very fast, even with 100 neighbors, it is usually finished within 30 seconds. 
\subsubsection{Default implementation}
At first we started out using the default KNN implementation of Weka with default settings. The defaults are a \emph{LinearNN} search algorithm with the \emph{Eucledian distance} as distance function. We started out with increasing the amount of neighbors starting at one. Though with this settings we got the best results using just the nearest neighbor. 
\\\\
\begin{tabular}{ l | c |c }
\textbf{K} & \textbf{\% Correct} & \textbf{\% Wrong} \\
1 & 95.96 & 4.04 \\
2 & 94.925 & 5.075 \\
3 & 95.635 & 4.365 \\
\end{tabular}
\paragraph{}In order to improve the results we started with weighting the neighbors after their distance which helped us further improving the results. The best results where achieved by weighting the neighbors \emph{1/distance}. While the results yielded by the weighting function \emph{1-distance} where also better than without weighting, the were not as good.
\\\\
\begin{tabular}{ l | c | c }
\textbf{K} & \textbf{\% Correct} & \textbf{\% Wrong} \\
1 & 95.96 & 4.04 \\
2 & 95.99& 4.01\\
3 & 96.04 & 3.96 \\
\textbf{4} & \textbf{96.115} & \textbf{3.885} \\
5 & 96.03 & 3.97 \\
6 & 96.015 & 3.985 \\
8 & 95.75 & 4.25 \\
\end{tabular}
\paragraph{}With high values for K the prediction rates get worse. 
\\\\
\begin{tabular}{ l | c | c }
\textbf{K} & \textbf{\% Correct} & \textbf{\% Wrong} \\
40 & 90.915 & 9.085 \\
100 &  84.3 & 15.7 \\
\end{tabular}
\subsubsection{Manhattan Distance} did not improve our prediction, although again when the neighbors were weighted by their distance the success rate improved.
\subsubsection{Chebyshev Distance} also did not improve the result, as it only takes the distance between the attributes which are farthest away from each other as result. As for weighting again, the results improved slightly when using weighted neighbors.
\subsubsection{KD Tree Search Method} improves the run time of the classification significantly. The time for evaluating the models with 10-folds cross validation decreased by 66\%. 

\subsection{Random Forest}
The default settings for Random Forests are 100 Trees with unlimited depth, the number of randomly chosen features per tree is calculated by \textit{log\_2(\#predictors) + 1}.
\paragraph{Number of Trees} We started out with the default settings and varying the number of trees, starting at 100 trees. The result yielded was already at 96.41\% accuracy. Further increasing the number of trees improved the result continuously, but also increased the computation time. 
\\\\
\begin{tabular}{ c | c | c }
\textbf{\# Trees} & \textbf{\% Accuracy} & \textbf{Computation time (s)} \\
50 & 96.10\% & 3.1s \\
100 & 96.41\% & 6.55s \\
200 & 96.53\% & 12.33s \\
300 & 96.51\% & 19.13s \\
400 & 96.54\% & 24.46s \\
500 & 96.58\% & 30.66s\\
\end{tabular}

\paragraph{}Though the performance is improving with the number of trees, the improvements are getting smaller. While an increase of trees from 50 to 100 trees results in a gain of 0.3 percent, the increase from 400 to 500 trees only gains 0.04 percent accuracy. 
\subsubsection{Increasing Execution Slots}For computing a 10-fold cross-validation at 500 trees on one single thread, the taken time was 5:55min. In order to improve this, the number of execution slots (=threads) can be increased. When repeating the same operation with 10 slots our computation time improved to 1:56min. \\
Using 50 threads we calculated a model with 1000 trees, which yielded the best result so far with an accuracy of \emph{96.63\%}. 
\subsubsection{Modifying number of features per tree}
Through varying the number of features which will be selected per tree, we tried to increase the accuracy. \\
Results for \emph{\#Features: 3}:\\
\begin{tabular}{ c | c | c }
\textbf{\# Trees} & \textbf{\% Accuracy} \\
100 & 96.51\% \\
200 & 96.66\% \\
300 & 96.78\% \\
400 & 96.75\% \\
\end{tabular}

\subsubsection{Modifying tree depth}


\subsection{Decision Tree - 48J}
TODO\\
\subsection{Bayes Network}
TODO\\
\subsection{Conclusion}
TODO\\


% Section Data Set 2
\section{Data Set 2: Internet Advertisements}
This data set contains information on different banners/images on websites. The classification task on hand is to determine whether or not an instance is an advertisement or not. A possible use case for this data set would be to remove all adverts from a given website. Some attributes include the geometry of the images (if available), as well as phrases occurring in the URL, the image's URL and alt text, the anchor text and words near the anchor text.

The data set contains a total of 3279 instances, 2821 of which are non-advertisements, 458 are advertisements. Further the data set contains a total of 1558 attributes, 3 of which are continuous variables, the other ones all binary. 28\% of instances are missing one or more of the continuous variables.


\subsection{Preprocessing}
The data set, was contained in two files, one with file ending .name containing all the names of attributes, the other one with ending .data containing the data in a comma-separated format. As Weka prefers data in the ARFF format, we had to do some transformations on the data set prior to loading it into Weka. This was done by first adding the attribute names as header row into the .data file, and saving that file as a CSV file. This file could then be converted by Weka's CSV converter to get the desired ARFF format.

\subsection{K-Nearest Neighbours}
As first classification algorithm we used K-Nearest Neighbors. The parameters we experimented with were:

\begin{itemize}
\item The number of neighbors.
\item The distance weighting.
\item The search algorithm.
\item The distance function.
\end{itemize}

We did not use any missing value replacement strategy for this classifier, as the distance function used by K-NN can handle missing values. 

\textbf{Number of Neighbors:}
First, we experimented with the number of neighbors used by the classification algorithm. We did experiments with 1,2,3,4,5,6,8,10,20,50 and 100 neighbors.

\begin{tabular}{ c | c | c | c | c | c}
\textbf{\# of Neighbors} & \textbf{\% Correctly Classified} & \textbf{TP Rate Ad} & \textbf{TP Rate Non-Ad} & \textbf{F-Measure} & \textbf{ROC Area}\\
1   & 96.25\% & 0.854 & 0.98  & 0.966 & 0.943\\
2   & 95.58\% & 0.871 & 0.97  & 0.956 & 0.953\\
3   & 96.74\% & 0.804 & 0.994 & 0.966 & 0.957\\
4   & 96.68\% & 0.819 & 0.991 & 0.966 & 0.959\\
5   & 96.00\% & 0.747 & 0.995 & 0.958 & 0.965\\
6   & 96.00\% & 0.763 & 0.994 & 0.96  & 0.965\\
8   & 95.64\% & 0.710 & 0.996 & 0.953 & 0.967\\
10  & 95.27\% & 0.678 & 0.998 & 0.949 & 0.966\\
20  & 92.71\% & 0.486 & 0.999 & 0.916 & 0.967\\
50  & 88.9 \% & 0.211 & 0.999 & 0.857 & 0.957\\
100 & 88.2 \% & 0.161 & 0.999 & 0.843 & 0.959\\
\end{tabular}

Generally, we can observe that the classification task using K-NN rightly classifies the major part of instances. Depending on the number of neighbors up to almost 97\% of instances is classified correctly. The best results seem to emerge when the number of neighbors is fairly low, starting from 1 neighbor we see a slight increase in rightly classified instances up until 3 neighbors. After that the percentage of rightly classified instances starts to decrease again. Looking at the F-Measure our assumption that we find the best classification results by using around 3-4 neighbors is further strengthened as we see F-Measure values of 0.966 for each case. The ROC curve however tells a slightly different story. Here we can we the highest values for 8-20 neighbors, but the overall the values lie in a range between 0.943 and 0.967 and are therefore quite small.

Another interesting observation is that the true positive rates for class Ad seem to decrease with an increasing number of neighbors where as the true positive rate for class Non-ad seems to increase up to 0.999 meaning that almost all "Non-ad" instances are classified correctly. We assume this is because our "ad" instances do not lie all closely together in the data space with respect to their attributes, but rather scattered around the space in smaller groups. Our assumption is that only a few images of class "ad" have very similar attributes, and therefore returning great true-positive rates for a small number of neighbors, whereas with more neighbors, the classifier has to deal with more images that are also quite similar to the given image but of different classes. As we have way more instances of class "non-ad" this comes down to a "majority decision" leaving little to no chance to the classifier but to classify the given instance as "non-ad", therefore explaining the drastic decrease in the true-positive rate for class "ad" and the increase of the same for class "non-ad".

For 2 - 4 neighbors we generally got very good classification results, which is why we think the "ideal" number of neighbors for k-NN lies in that region. However, you cannot choose the ideal number of neighbors without defining a clear idea of what you want to achieve. This may be different depending on the context. For example, looking at the general performance (F-Measure, ROC-Curve) we may conclude that 2-4 neighbors is ideal for the given data set. However, in the context of "ad" and "non-ad" images on web pages, one might want to achieve a high number of identified ads while keeping the number of non-ad images falsely classified  as advertisements to a minimum as these might show important information to the user. In this case it might be better to set the number of neighbors to 10 or 20 as this will yield very high true positive rates for class "non-ad" and fairly good true positive rates for class "ad". 

\textbf{Distance Weighting:}
Next, we experimented with different distance weighting settings. In Weka one can choose between no distance weighting, 1/distance weighting, and 1-distance weighting. Like we discussed in the section before, 2-4 neighbors generally showed "the best" results in the classification, which is why we did the following experiments all using 2-4 neighbors as well as 10 and 100 neighbors just for comparison. Below are the results of the classification task:

\begin{tabular}{c | c | c | c | c | c | c}
\textbf{Distance Weighting} & \textbf{\# of Neighbors} & \textbf{\% Correctly Classified} & \textbf{TP Rate Ad} & \textbf{TP Rate Non-Ad} & \textbf{F-Measure} & \textbf{ROC Area}\\
No distance & 2   & 95.58\% & 0.871 & 0.97  & 0.956 & 0.953\\
1/distance  & 2   & 96.43\% & 0.854 & 0.982 & 0.964 & 0.964\\
1-distance  & 2   & 96.46\% & 0.856 & 0.982 & 0.964 & 0.963\\
No distance & 3   & 96.74\% & 0.804 & 0.994 & 0.966 & 0.957\\
1/distance  & 3   & 96.74\% & 0.808 & 0.993 & 0.966 & 0.967\\
1-distance  & 3   & 96.71\% & 0.800 & 0.994 & 0.966 & 0.964\\
No distance & 4   & 96.68\% & 0.819 & 0.991 & 0.966 & 0.959\\
1/distance  & 4   & 96.71\% & 0.804 & 0.994 & 0.966 & 0.97\\
1-distance  & 4   & 96.71\% & 0.800 & 0.994 & 0.966 & 0.966\\
No distance & 10  & 95.27\% & 0.678 & 0.998 & 0.949 & 0.966\\
1/distance  & 10  & 95.73\% & 0.712 & 0.997 & 0.954 & 0.973\\
1-distance  & 10  & 95.18\% & 0.671 & 0.998 & 0.948 & 0.97\\
No distance & 100 & 88.2 \% & 0.161 & 0.999 & 0.843 & 0.959\\
1/distance  & 100 & 90.18\% & 0.305 & 0.999 & 0.879 & 0.972\\
1-distance  & 100 & 88.2 \% & 0.161 & 0.999 & 0.843 & 0.963\\

TODO: Observation & Conclusion

\subsection{Random Forest}
TODO\\
\subsection{Decision Tree - 48J}
TODO\\
\subsection{Bayes Network}
TODO\\
\subsection{Conclusion}
TODO\\


% Section Data Set 3
\section{Data Set 3: Leukemia}
TODO\\
\subsubsection{Preprocessing}
TODO what preprocessing was done\\
\subsubsection{K-Nearest Neighbours}
TODO\\
\subsubsection{Random Forest}
TODO\\
\subsubsection{Decision Tree - 48J}
TODO\\
\subsubsection{Bayes Network}
TODO\\
\subsubsection{Conclusion}
TODO\\


% Section Data Set 4
\section{Data Set 4: Congressional Voting}
TODO\\
\subsubsection{Preprocessing}
TODO what preprocessing was done\\
\subsubsection{K-Nearest Neighbours}
TODO\\
\subsubsection{Random Forest}
TODO\\
\subsubsection{Decision Tree - 48J}
TODO\\
\subsubsection{Bayes Network}
TODO\\
\subsubsection{Conclusion}
TODO\\


\section{Conclusion}
TODO\\



\end{document}
