% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

%\documentclass{sig-alternate-05-2015}
\documentclass{article}
\usepackage{caption}

\begin{document}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Classification Tasks}
%\subtitle{Classification Tasks}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

%\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{ML Exercise 2 - Group 19}
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
%\alignauthor
%Lukas Stanek \\
%       \affaddr{Technical University of Vienna}\\
%       \affaddr{Karlsplatz 13}\\
%       \affaddr{1040, Vienna}\\
%       \email{----}
%% 2nd. author
%Thomas Appler \\
%       \affaddr{Technical University of Vienna}\\
%       \affaddr{Karlsplatz 13}\\
%       \affaddr{1040, Vienna}\\
%       \email{----}
%% 3rd. author
%\alignauthor
%Marten Sigwart \\
%       \affaddr{Technical University of Vienna}\\
%       \affaddr{Karlsplatz 13}\\
%       \affaddr{1040, Vienna}\\
%       \email{e1638152@student.tuwien.ac.at}
%}
% There's nothing stop
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{center}
\begin{tabular}{l| c r}
Group & Lukas Stanek & 1027203 \\ % Partner names
& Thomas Appler \\
& Marten Sigwart & 1638152 \\
\end{tabular}
\end{center}

\section{Introduction}
Our task was to apply 4 different classification algorithms to 4 different data sets. Hereby, we had to experiment with different parameter settings of the classification algorithm, evaluate the performance by choosing appropriate performance measures and compare results among classifiers and data sets. We also had to perform some pre-processing steps on the data prior to applying the classifiers and study the impact of those pre-processing methods.

\subsection{4 data sets}
We chose four different data sets for the task. We chose data sets which varied in size, varied in the number of attributes and varied in the number of instances. Further we tried to pick data sets with different numbers of classes. Some data sets had missing values to take into account.

The four data sets we chose are:

\begin{itemize}
\item \textbf{Letter Recognition}
This data set contained attributes about letters shown on a rectangular, black and white display. The classification task is to determine the right letter (A-Z)
\item \textbf{Internet Advertisements}
This data set contained information about images of different web pages. The classification task is to determine whether or not an image is an advertisement.
\item \textbf{Leukemia}
This data set contained medical information of cancer patients. The classification task is to determine whether or not a certain patients has cancer.
\item \textbf{Congressional Voting}
This data set contains information on different voters. The classification task is to determine the party affiliation of the voters.
\end{itemize}

Each data set will be described in more detail in sections below.


\subsection{4 classification algorithms}
We chose 4 different classifiers for the task. Hereby, it was important that at least 3 of the classifiers derived from totally different learning algorithms. The classifiers we chose are:

\begin{itemize}
\item \textbf{K-Nearest Neighbors}
\item \textbf{Random Forrest}
\item \textbf{Decision Tree}
\item \textbf{Bayes Network}
\end{itemize}


\subsubsection{Algorithm 1: K-Nearest Neighbors}
KNN itself doesn't deal with missing values, it's the distance 
function. E.g., in case of euclidean distance, the following applies 
to missing values while computing the distance of the same attribute 
between two weka.core.Instance objects: 
- attributes in both Instance objects have missing value 
  => 0 
- only one value is missing 
  ~ nominal attribute 
    => 1 
  ~ numeric attribute 
    => 1 in case of normalization, maxRange-minRange for that 
attribute otherwise 
TODO\\
\subsubsection{Random Forest: Random Forrest}
TODO\\
\subsubsection{Decision Tree - 48J: Decision Tree}
TODO\\
\subsubsection{Bayes Network: Bayes??}
TODO\\

\subsection{Performance Measures}
TODO\\

% Section Data Set 1
\section{Data Set 1: Letter Recognition}
The objective for this data set is to predict a letter shown on a rectangular, black and white, display. There are 16 numerical features  provided for determining the correct letter. These attributes provide information about statistical properties of the letter like total number of pixels, width, mean of x-axis pixels, etc.
All of these values were then scaled into a range from 0 through 15. The 26 capital letters to predict in this set were taken from 20 different fonts. The letters are more or less evenly spread over the 20.000 instances, within a range of 734 to 813 occurrences. This data set contains no missing values. For comparable results 10-fold cross validation was used to measure the algorithms precision.
As for performance criteria with this data set only the percentage of correctly classified instances will be considered. The reason for this being that if a letter is wrongly classified, it doesn't matter which other letter was recognized instead of the correct one. 
\\
\subsection{Preprocessing}
As the data set was only provided as CSV file without header row, we added a header row for displaying the attribute names in Weka and therefore facilitating the analysis of the results. 
\\
\subsection{K-Nearest Neighbors}
In order to be able to compare the results we always used the same number of neighbors: 1,2,3,4,5,6,8,10,16,20,40,100.\\
The algorithm is generally very fast, even with 100 neighbors, it is usually finished within 30 seconds. 
\subsubsection{Default implementation}
At first we started out using the default KNN implementation of Weka with default settings. The defaults are a \emph{LinearNN} search algorithm with the \emph{Eucledian distance} as distance function. We started out with increasing the amount of neighbors starting at one. Though with this settings we got the best results using just the nearest neighbor. 
\\\\
\begin{tabular}{ l | c |c }
\textbf{K} & \textbf{\% Correct} & \textbf{\% Wrong} \\
1 & 95.96 & 4.04 \\
2 & 94.925 & 5.075 \\
3 & 95.635 & 4.365 \\
\end{tabular}
\paragraph{}In order to improve the results we started with weighting the neighbors after their distance which helped us further improving the results. The best results where achieved by weighting the neighbors \emph{1/distance}. While the results yielded by the weighting function \emph{1-distance} where also better than without weighting, the were not as good.
\\\\
\begin{tabular}{ l | c | c }
\textbf{K} & \textbf{\% Correct} & \textbf{\% Wrong} \\
1 & 95.96 & 4.04 \\
2 & 95.99& 4.01\\
3 & 96.04 & 3.96 \\
\textbf{4} & \textbf{96.115} & \textbf{3.885} \\
5 & 96.03 & 3.97 \\
6 & 96.015 & 3.985 \\
8 & 95.75 & 4.25 \\
\end{tabular}
\paragraph{}With high values for K the prediction rates get worse. 
\\\\
\begin{tabular}{ l | c | c }
\textbf{K} & \textbf{\% Correct} & \textbf{\% Wrong} \\
40 & 90.915 & 9.085 \\
100 &  84.3 & 15.7 \\
\end{tabular}
\subsubsection{Manhattan Distance} did not improve our prediction, although again when the neighbors were weighted by their distance the success rate improved.
\subsubsection{Chebyshev Distance} also did not improve the result, as it only takes the distance between the attributes which are farthest away from each other as result. As for weighting again, the results improved slightly when using weighted neighbors.
\subsubsection{KD Tree Search Method} improves the run time of the classification significantly. The time for evaluating the models with 10-folds cross validation decreased by 66\%. 

\subsection{Random Forest}
The default settings for Random Forests are 100 Trees with unlimited depth, the number of randomly chosen features per tree is calculated by \textit{log\_2(\#predictors) + 1}.
\paragraph{Number of Trees} We started out with the default settings and varying the number of trees, starting at 100 trees. The result yielded was already at 96.41\% accuracy. Further increasing the number of trees improved the result continuously, but also increased the computation time. 
\\\\
\begin{center}
\begin{tabular}{ c | c | c }
\textbf{\# Trees} & \textbf{\% Accuracy} & \textbf{Computation time (s)} \\
50 & 96.10\% & 3.1s \\
100 & 96.41\% & 6.55s \\
200 & 96.53\% & 12.33s \\
300 & 96.51\% & 19.13s \\
400 & 96.54\% & 24.46s \\
500 & 96.58\% & 30.66s\\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center}

\paragraph{}Though the performance is improving with the number of trees, the improvements are getting smaller. While an increase of trees from 50 to 100 trees results in a gain of 0.3 percent, the increase from 400 to 500 trees only gains 0.04 percent accuracy. 
\subsubsection{Increasing Execution Slots}For computing a 10-fold cross-validation at 500 trees on one single thread, the taken time was 5:55min. In order to improve this, the number of execution slots (=threads) can be increased. When repeating the same operation with 10 slots our computation time improved to 1:56min. \\
Using 50 threads we calculated a model with 1000 trees, which yielded the best result so far with an accuracy of \emph{96.63\%}. 
\subsubsection{Modifying number of features per tree}
Through varying the number of features which will be selected per tree, we tried to increase the accuracy. \\
Results for \emph{\#Features: 3}:\\
\begin{center}
\begin{tabular}{ c | c | c }
\textbf{\# Trees} & \textbf{\% Accuracy} \\
100 & 96.51\% \\
200 & 96.66\% \\
300 & 96.78\% \\
400 & 96.75\% \\
500 & 96.75\% \\
\end{tabular}
\captionof{table}{Results for 3-feature trees}
\end{center}
The best results were achieved with a number of 3 features per tree, other numbers only yielded worse results.

\subsubsection{Modifying tree depth}
Through modifying the maxial depth a tree is allowed to have we also tried to improve the results of our predictions.We observed that when reducing the depth drastically to something below 10, the prediction rate drops drastically. 

\begin{center}

\begin{tabular}{c | c }
\textbf{Max depth} & \textbf{Accuracy}\\
3 & 56.49\%\\
6 & 73.97\%
\end{tabular}
\captionof{table}{Results below 10}
\end{center}

But when raising the depth to 20 - 30 the rates start to normalize again also clieb a bit higher than on runs with the default settings.
\begin{center}

\begin{tabular}{c | c }
\textbf{Max depth} & \textbf{Accuracy}\\
25 & 96.415\%\\
26 & 96.42\%\\
27 & 96.42\%\\
28 & 96.41\%\\
\end{tabular}
\captionof{table}{Results above 25}
\end{center}

\subsubsection{Best combination}
The best combination of settings we found was a Tree depth of 27 and the number of features set to 3. The result was a accuracy of \textbf{96.775\%}.
\subsection{Decision Tree - 48J}
As for this implementation of the decision tree we also started with the default settings for this data set. The first results were significantly worse compared to the other algorithms. The prediction accuracy was down to 87.92\% and also the ROC Area had dropped to 0.954 (The other algorithms were basically at 0.99 or 1). The tree had 1226 leafs and a size of 2451.
\subsubsection{Number of objects per leaf}
At first we tried to modify the minimum number of objects per leaf. With the default being two we tried to set it to one, which increased the number of leafs to 1771 and the size of the tree to 3541. The result was slightly better but it seemed like overfitting. \\
Although increasing the number of minimum nodes per tree resulted in worse results.\\

\begin{center}
\begin{tabular}{ c | c | c | c}
\textbf{Obj. per leaf} & \textbf{\% Accuracy} & \textbf{\# Leafs} & \textbf{Treesize}\\
\hline
1 & 88.66\% & 1771 & 3541\\
2 & 87.92\% & 1226 & 2451\\
3 & 87.20\% & 1015 & 2029\\
4 & 86.36\% & 852 & 1703\\
5 & 85.82\% & 750 & 1499\\
8 & 84.07\% & 563 & 1125\\
\end{tabular}
\captionof{table}{Results for varying obj per leaf}
\end{center}

\subsubsection{Pruning}
Pruning helps to prevent overfitting a decison tree, so that it is specialized to one dataset and does not generalize well. We adjusted a setting called confidence factor to influence the amount of pruning done during model creation. A higher confidence factor results in less pruning.


\begin{center}
\begin{tabular}{ c | c | c | c}
\textbf{Confidence Factor} & \textbf{\% Accuracy} & \textbf{\# Leafs} & \textbf{Treesize}\\
\hline
0.1 & 87.85\% & 1138 & 2275\\
0.2 & 87.88\% & 1198 & 2395\\
0.3 & 87.96\% & 1255 & 2509\\
0.4 & 87.94\% & 1255 & 2509\\
0.5 & 87.96\% & 1267 & 2533\\
0.6 & 87.98\% & 1303 & 2605\\
\end{tabular}
\captionof{table}{Ajusting the confidence factor}
\end{center}

The accuracy is increasing when pruning is decreased, but this would also increase the chances of the tree being overfitted.\\
It is also possible to change the pruning mode from C4.5 pruning to reduced-error pruning. It yields slighly worse results than the default pruning method.


\subsection{Bayes Network}
When using da Bayes Network implementation in Weka we can adjust 2 settings, the \emph{estimator} and the \emph{searchAlgorithm}. With the default settings, which use the K2 search algorithm, the result is an accuracy of \emph{74.31\%}. We were able to improve the prediction by increasing the size of parents every network node is allowed to have.

\begin{center}
\begin{tabular}{ c | c }
\textbf{Max \# parents} & \textbf{\% Accuracy} \\
\hline
1  & 74.31\% \\
2  & 83.69\% \\
3  & 86.72\% \\
4  & 86.76\% \\
5  & 86.76\% \\
10 & 86.76\% \\
\end{tabular}
\captionof{table}{Ajusting the ne maximum number of parents of a node}
\end{center}

\paragraph{}After using a random order of node in the network the results got slightly better, but unfortionatly the random function takes no seed, so that the results vary over time.

\begin{center}
\begin{tabular}{ c | c }
\textbf{Round} & \textbf{\% Accuracy} \\
\hline
1  & 88.455\% \\
2  & 88.26\% \\
3  & 88.46\% \\
4  & 88.185\% \\
5  & 87.88\% \\
\end{tabular}
\captionof{table}{Accuracy of random node order}
\end{center}

\subsubsection{HillClimber}
By using the Hill climber algorithm we could achieve once again better results. But we needed to raise the number of parent nodes agian.
\begin{center}
\begin{tabular}{ c | c }
\textbf{\# Parents} & \textbf{\% Accuracy} \\
\hline
1  & 74.31\% \\
4  & 89.45\% \\
6  & 89.45\% \\
\end{tabular}
\captionof{table}{Accuracy HillClimber search algorithm}
\end{center}

We could achieve similar results with the other Hillclimber algorithms \emph{LAGDHillclimber} and \emph{Repeated HillClimber} . They also achieved results of 89\% and above.

\subsubsection{Tabu search}
When using the Tabu Search algorithm we could also improve our results by increasing the number of runs and the length of the tabu list.
\begin{center}
\begin{tabular}{ c | c  | c | c }
\textbf{\# Parents} & \textbf{\# Runs} & \textbf{Length Tabu list}  & \textbf{\% Accuracy} \\
\hline
1  & 10 & 5 & 74.31\% \\
4  & 10 & 5 & 84.26\% \\
4  & 100 & 10&  89.45\% \\
\end{tabular}
\captionof{table}{Accuracy Tabu search algorithm}
\end{center}

\paragraph{}So for Bayes Networks and this data set you should be using HillClimber based algorithms as they perform slighlty better than a K2. We could not test the genetic search algorithm as the computation time was to long.

\subsection{Conclusion}
\subsubsection{Letter mix-up}
When predicting letters there are some which are pretty well distinguishable from one another and some are not. So it happens that its easy to confuse two letters which look similar to each other. This also applys to our machine learning algorithms. Some letters have a worse prediction rate than others. We can tell which characters are often confused by looking at the confusion matrix. It tells us how often a character was classified correctly or if not for which letter it was mistaken. This allows us also to tell whether the confusion is only going one-way. Meaning that one character is often mistaken for another one, but the other mostly classified correctly.\\
The top candiates for confusion are:\\
\begin{center}
\begin{tabular}{ c c }
\hline
J & I \\
P & F \\
H & K \\
Q & O \\
\hline
\end{tabular}
\end{center}
\paragraph{}\emph{J} and \emph{I} are mistaken with each other equally often, it seems to be generally harder to distinguish these two letters. This also seems to be logical as they look very similar and their geometrical properties could be very similar.\\
Interestingly why \emph{Q} is often mistaken for \emph{O}, \emph{O} is much less mistaken for \emph{Q}. The same can be said about \emph{P} and \emph{F}, where \emph{P} is more often mistaken for \emph{F}, and \emph{H} and \emph{K}.


% Section Data Set 2
\section{Data Set 2: Internet Advertisements}
This data set contains information on different banners/images on websites. The classification task on hand is to determine whether or not an instance is an advertisement or not. A possible use case for this data set would be to remove all adverts from a given website. Some attributes include the geometry of the images (if available), as well as phrases occurring in the URL, the image's URL and alt text, the anchor text and words near the anchor text.

The data set contains a total of 3279 instances, 2821 of which are non-advertisements, 458 are advertisements. Further the data set contains a total of 1558 attributes, 3 of which are continuous variables, the other ones all binary. 28\% of instances are missing one or more of the continuous variables.


\subsection{Preprocessing}
The data set, was contained in two files, one with file ending .name containing all the names of attributes, the other one with ending .data containing the data in a comma-separated format. As Weka prefers data in the ARFF format, we had to do some transformations on the data set prior to loading it into Weka. This was done by first adding the attribute names as header row into the .data file, and saving that file as a CSV file. This file could then be converted by Weka's CSV converter to get the desired ARFF format.

\subsection{K-Nearest Neighbours}
As first classification algorithm we used K-Nearest Neighbors. The parameters we experimented with were:

\begin{itemize}
\item The number of neighbors.
\item The distance weighting.
\item The search algorithm.
\item The distance function.
\end{itemize}

We did not use any missing value replacement strategy for this classifier, as the distance function used by K-NN can handle missing values. 

\textbf{Number of Neighbors:}
First, we experimented with the number of neighbors used by the classification algorithm. We did experiments with 1,2,3,4,5,6,8,10,20,50 and 100 neighbors.


\begin{tabular}{ c | c | c | c | c | c}
\textbf{\# of Neighbors} & \textbf{\% Correctly Classified} & \textbf{TP Rate Ad} & \textbf{TP Rate Non-Ad} & \textbf{F-Measure} & \textbf{ROC Area}\\
1   & 96.25\% & 0.854 & 0.98  & 0.966 & 0.943\\
2   & 95.58\% & 0.871 & 0.97  & 0.956 & 0.953\\
3   & 96.74\% & 0.804 & 0.994 & 0.966 & 0.957\\
4   & 96.68\% & 0.819 & 0.991 & 0.966 & 0.959\\
5   & 96.00\% & 0.747 & 0.995 & 0.958 & 0.965\\
6   & 96.00\% & 0.763 & 0.994 & 0.96  & 0.965\\
8   & 95.64\% & 0.710 & 0.996 & 0.953 & 0.967\\
10  & 95.27\% & 0.678 & 0.998 & 0.949 & 0.966\\
20  & 92.71\% & 0.486 & 0.999 & 0.916 & 0.967\\
50  & 88.9 \% & 0.211 & 0.999 & 0.857 & 0.957\\
100 & 88.2 \% & 0.161 & 0.999 & 0.843 & 0.959\\
\end{tabular}


Generally, we can observe that the classification task using K-NN rightly classifies the major part of instances. Depending on the number of neighbors up to almost 97\% of instances is classified correctly. The best results seem to emerge when the number of neighbors is fairly low, starting from 1 neighbor we see a slight increase in rightly classified instances up until 3 neighbors. After that the percentage of rightly classified instances starts to decrease again. Looking at the F-Measure our assumption that we find the best classification results by using around 3-4 neighbors is further strengthened as we see F-Measure values of 0.966 for each case. The ROC curve however tells a slightly different story. Here we can we the highest values for 8-20 neighbors, but the overall the values lie in a range between 0.943 and 0.967 and are therefore quite small.

Another interesting observation is that the true positive rates for class Ad seem to decrease with an increasing number of neighbors where as the true positive rate for class Non-ad seems to increase up to 0.999 meaning that almost all "Non-ad" instances are classified correctly. We assume this is because our "ad" instances do not lie all closely together in the data space with respect to their attributes, but rather scattered around the space in smaller groups. Our assumption is that only a few images of class "ad" have very similar attributes, and therefore returning great true-positive rates for a small number of neighbors, whereas with more neighbors, the classifier has to deal with more images that are also quite similar to the given image but of different classes. As we have way more instances of class "non-ad" this comes down to a "majority decision" leaving little to no chance to the classifier but to classify the given instance as "non-ad", therefore explaining the drastic decrease in the true-positive rate for class "ad" and the increase of the same for class "non-ad".

For 2 - 4 neighbors we generally got very good classification results, which is why we think the "ideal" number of neighbors for k-NN lies in that region. However, you cannot choose the ideal number of neighbors without defining a clear idea of what you want to achieve. This may be different depending on the context. For example, looking at the general performance (F-Measure, ROC-Curve) we may conclude that 2-4 neighbors is ideal for the given data set. However, in the context of "ad" and "non-ad" images on web pages, one might want to achieve a high number of identified ads while keeping the number of non-ad images falsely classified  as advertisements to a minimum as these might show important information to the user. In this case it might be better to set the number of neighbors to 10 or 20 as this will yield very high true positive rates for class "non-ad" and fairly good true positive rates for class "ad". 

\textbf{Distance Weighting:}
Next, we experimented with different distance weighting settings. In Weka one can choose between no distance weighting, 1/distance weighting, and 1-distance weighting. Like we discussed in the section before, 2-4 neighbors generally showed "the best" results in the classification, which is why we did the following experiments all using 2-4 neighbors as well as 10 and 100 neighbors just for comparison. Below are the results of the classification task:

\begin{tabular}{c | c | c | c | c | c | c}
\textbf{Distance Weighting} & \textbf{\# of Neighbors} & \textbf{\% Correctly Classified} & \textbf{TP Rate Ad} & \textbf{TP Rate Non-Ad} & \textbf{F-Measure} & \textbf{ROC Area}\\
No distance & 2   & 95.58\% & 0.871 & 0.97  & 0.956 & 0.953\\
1/distance  & 2   & 96.43\% & 0.854 & 0.982 & 0.964 & 0.964\\
1-distance  & 2   & 96.46\% & 0.856 & 0.982 & 0.964 & 0.963\\
No distance & 3   & 96.74\% & 0.804 & 0.994 & 0.966 & 0.957\\
1/distance  & 3   & 96.74\% & 0.808 & 0.993 & 0.966 & 0.967\\
1-distance  & 3   & 96.71\% & 0.800 & 0.994 & 0.966 & 0.964\\
No distance & 4   & 96.68\% & 0.819 & 0.991 & 0.966 & 0.959\\
1/distance  & 4   & 96.71\% & 0.804 & 0.994 & 0.966 & 0.97\\
1-distance  & 4   & 96.71\% & 0.800 & 0.994 & 0.966 & 0.966\\
No distance & 10  & 95.27\% & 0.678 & 0.998 & 0.949 & 0.966\\
1/distance  & 10  & 95.73\% & 0.712 & 0.997 & 0.954 & 0.973\\
1-distance  & 10  & 95.18\% & 0.671 & 0.998 & 0.948 & 0.97\\
No distance & 100 & 88.2 \% & 0.161 & 0.999 & 0.843 & 0.959\\
1/distance  & 100 & 90.18\% & 0.305 & 0.999 & 0.879 & 0.972\\
1-distance  & 100 & 88.2 \% & 0.161 & 0.999 & 0.843 & 0.963\\
\end{tabular}

\paragraph{}For just 2 neighbors we can see a slight increase in correctly classified instances, also the F-Measure and ROC Area seem to hint at an improved classifier. However, for 3 and 4 neighbors all classifiers seem to show very similar results, so if the distance weighting has an impact, it is now so low that it is basically negligible. We observe the same picture while looking at the results for 10 or 100 neighbors. In general, the F-Measure and ROC Area show very similar results. Only the inverse distance weighting seems to slightly improve the classification on all experiments, which might indicate a more robust method for classification. 

So even though the overall performance of the classifier does not seem to be affected that much by the distance weighting we can, however, make out a factor where it does have an impact. In the section above we looked at different numbers of neighbors and we noticed that the true-positive rate for class "ad" seems to decrease when we increase the number of neighbors. We explained this by only a few very similar instances of the class "ad" lying closely together in the data space while there are still quite a few similar instances but of class "non-ad" also in the neighborhood, which leads to some kind of majority count while classifying. The same behavior can also be observed when using distance weighting, though, the decrease of the true-positive rate is not as strong as for no distance weighting. We think this confirms our assumption that the immediate neighbors of an instance of class "ad" are also of class "ad". When using distance weighting within the classifier these instances now get assigned a bigger weight which enhances the probability that the given instance will also be classified as "ad". This reduces the effect of the majority count we observed when not using any distance weighting.

In conclusion, we can say while the distance weighting does not seem to have any strong impact on the general performance, one might still make use of it when the classes are not fairly balanced. E.g. using inverse distance weighting on the classifier might reduce the effect of the majority count when using more neighbors.


\subsection{Random Forest}
TODO\\
\subsection{Decision Tree - 48J}
TODO\\
\subsection{Bayes Network}
TODO\\
\subsection{Conclusion}
TODO\\


% Section Data Set 3
\section{Data Set 3: Leukemia}
TODO\\
\subsubsection{Preprocessing}
TODO what preprocessing was done\\
\subsubsection{K-Nearest Neighbours}
TODO\\
\subsubsection{Random Forest}
TODO\\
\subsubsection{Decision Tree - 48J}
TODO\\
\subsubsection{Bayes Network}
TODO\\
\subsubsection{Conclusion}
TODO\\


% Section Data Set 4
\section{Data Set 1: Congressional Voting}

This data set contains 16 votes on different topics (like immigration, education spending) for each of the US House of representatives Congressmen. Those 16 votes were identified by the CQA as the 16 key votes. The CQA lists nine different types of votes, which are simplified here to yea (voted for, paired for, announced for), nay (voted against, paired against, announced against) and unknown (voted present, voted present to avoid conflict of interest, no vote or unknown position). The objective here is now to predict, whether a person is republican or democrat. 
So in total, the data set is quite small and contains 18 attributes (id, class and 16 topics) and 218 instances. 

\subsection{Preprocessing}
 
First we removed the attribute "id", due to the fact that attributes with unique values do not contain useful information. The second step was to replace all values "unknown" with "?", so that Weka is able to interpret this as missing values, and not as nominal values. To treat the missing values, we used the preprocessing command "ReplaceMissingValues", which replaces every "?" with the mode, that means with the value which appears the most.
Another approach can be to first transform all nominal values to numerical zeros and ones and perform then the “ReplaceMissingValues” command. This way, all missing values are not replaced with the mode of the attribute, but with the mean. This means that these values may be between zero and one. But it turned out, that this approach decreases our prediction accuracy in most cases, therefore we will not use this method.
Another approach could be also to remove all instances with missing values. But in this case it does not make much sense, because the rate of missing values is quite high and the size of the set is very small. Therefore we would eliminate too much instances. 
 

\subsection{kNN}
 

\subsubsection{parameter k}
 
With default setting we first tested the data set with an 10-fold cross validation with different values for “k” and it turned out, that three gives us the best accuracy rate of 93.12\%. But also with values 6 and 7 for k the results are quite good. In general it looks like every value for k works quite good for this data set. Although the prediction rate gets worse with high values for k, the decrease is not that big. For example if k is 40, the accuracy decreases just about 3 percent. Also for k equal 1 the result is very good (91.74\%). Considering that that data set is very small, this means that just 3 instances of 218 were misclassified more than with k equals 3. It looks like, that except some outliers, the data is relatively good seperated, because about 92\% of the instances have a nearest neighbour with the same class. An interesting observation is also that the weighted ROC area stays very high, also for high values for k. For example for values 16-40, the weighted ROC area is even higher, that the ROC area with k equals 3. After that we also tried the two different weighting approaches, but they had no impact on the results. All results with different k's are shown the following table.
 
 \begin{center}
\begin{tabular}{ c | c | c | c }
\textbf{k} & \textbf{\% Accuracy} & \textbf{F-Measure} & \textbf{ROC area} \\
1 & 91.7431 & 0.918 & 0.955 \\
2 & 91.7431 & 0.918 & 0.962 \\
3 & 93.1193 & 0.932 & 0.974 \\
4 & 91.7431 & 0.918 & 0.979 \\
5 & 91.2844 & 0.914 & 0.981 \\
6 & 92.2018 & 0.923 & 0.972 \\
7 & 92.2018 & 0.923 & 0.973 \\
8 & 91.7431 & 0.918 & 0.976 \\
10 & 91.2844 & 0.914 & 0.976 \\
16 & 91.2844 & 0.914 & 0.981 \\
20 & 91.2844 & 0.914 & 0.982 \\
40 & 89.9083 & 0.900 & 0.981 \\
100 & 88.0734 & 0.882 & 0.969 \\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center}
 
\subsubsection{Distance Functions and Search Methods}
 
Except the “chebyshev” distance function, all distance functions lead to the same result. With the “chebyshev” distance the accuracy got a lot worse and decreased to 72.48\%. When weighting the the neighbours, the “manhattan” distance function decreased the precision rate, but just to 90.82\%. Due to the fact that there are just 218 instances, other search methods that thee default one do not really have a noticeable effect on performance.
 
\subsection{Random Forest}
 
\subsubsection{Number of trees}
 
With default settings we first started to vary the number of trees and it turned out, that 16 gave us the best result of 97.25\% accuracy and the lower the number is, the worse the result. As expected, because of the small data set the ideal number is quit low. But also with a higher number of trees the classifier yielded almost the same resault (96.789\% accuracy). But this difference is not significantly worse, because just one instance more was misclassified. This means that increasing the number of trees improves the result, until a point, where it stays the same.
 
\begin{center}
\begin{tabular}{ c | c | c | c }
\textbf{num. trees} & \textbf{\% Accuracy} & \textbf{F-Measure} & \textbf{ROC area} \\
100 & 96.789 & 0.968 & 0.996 \\
50 & 96.789 & 0.968 &  0.996 \\
20 & 96.789 & 0.968 & 0.995 \\
17 & 96.789 & 0.968 & 0.994 \\
16 & 97.2477 & 0.973 & 0.994 \\
14 & 95.8716 & 0.959 & 0.994 \\
8 & 95.4128 & 0.968 & 0.992 \\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center}
 
\subsubsection{Number of Attributes}
 
The default number of attributes until now was 5 (log2(#attributes)+1). After testing with different numbers, it turned out, that just very low values (<4) and high values (>14) lead to worse results. Others in between lead to almost the same result.
 
\begin{center}
\begin{tabular}{ c | c | c | c }
\textbf{num of. att.} & \textbf{\% Accuracy} & \textbf{F-Measure} & \textbf{ROC area} \\
16 & 95.8716 & 0.959 & 0.987  \\
15 & 95.8716 & 0.959 & 0.987 \\
14 & 96.789 &  0.968 & 0.989 \\
12 & 97.2477 & 0.973 & 0.992 \\
10 & 97.7064 & 0.977 & 0.989 \\
8 & 97.2477 & 0.973 & 0.992 \\
5 & 97.2477 & 0.973 & 0.992 \\
4 & 96.789 & 0.968 & 0.995 \\
3 & 96.3303 & 0.963 & 0.989 \\
2 & 94.9541 & 0.950 & 0.992 \\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center} 

\subsubsection{Tree depth}
 
In the default settings, an unlimited depth of trees is possible. As expected, limiting the maximal depth of trees did not improve our results. If maximal tree depth is smaller than 8, the result is worse, if it is 8 or more, the result stays the same.
 
\begin{center}
\begin{tabular}{ c | c | c | c }
\textbf{max depth} & \textbf{\% Accuracy} & \textbf{F-Measure} & \textbf{ROC area} \\
2 & 96.3303 & 0.963 & 0.990 \\
4 & 96.789 & 0.968 & 0.991 \\
6 & 97.2477 & 0.973 & 0.988 \\
8 & 97.7064 & 0.977 & 0.989 \\
20 & 97.7064 & 0.977 & 0.989 \\
100 & 97.7064 & 0.977 & 0.989 \\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center}
 
\subsection{Decision tree}
 
\subsubsection{Pruning}
 
First we compared the results of the decision tree with and without pruning. We used the default settings and gained an increase of 1.38\% accuracy with pruning. Next we tested the parameter confidence factor, which is used for pruning. It turned out, that with a confidence factor from 0.25 to 0.5, the result stays the same. But if this factor is less than 0.25 or higher than 0.5 the result gets worse, although the difference is not that big. In general, pruning improves the results, but not significantly. Usually the problem of an unpruned tree is, that it is too specialized. But in our case, this is not really the case. Just 3 instances more are misclassified, than with an pruned tree. 
 
\begin{center}
\begin{tabular}{ c | c | c | c }
\textbf{pruned} & \textbf{confidence fac.} & \textbf{\% Accuracy} & \textbf{F-Measure} & \textbf{ROC area} \\
no & - & 95.4128 & 0.954 & 0.969 \\
yes & 0.1 & 96.3303 & 0.963 & 0.943 \\
yes & 0.25 & 96.789 & 0.968 & 0.952 \\	
yes & 0.50 & 96.789 & 0.968 & 0.952 \\
yes & 0.55 & 95.4128 & 0.954 & 0.969 \\
yes & 0.70 & 95.4128 & 0.954 & 0.969 \\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center} 

\subsubsection{Other parameters}
 
Changing other parameters like “collapseTree”, “useLaplace” or “numFolds” had no impact on the results in this case. The only other parameter which made a noticeable difference was minNumObj. If this parameter is not set to 2 or 3, the results get worse.
 
 
\subsection{Bayesian Network}
 
\subsubsection{Search algorithm}
 
First we started to test the classifier with different search algorithms. With the default search algorithm “k2”, we gained an accuracy of 89.9\% and it turned out, that other methods like hillclimbing, repeated hillclimbing and “tabu search” lead to exactly the same result. The best result is yield with the TAN-search, which increased the accuracy rate to 95.41\%. Also the method simulated annealing leads to good results, but the performance is a lot worse. The runtime increased from about 0.02 seconds to almost two seconds. 
 

\begin{center}
\begin{tabular}{ c | c | c | c }
\textbf{Search algo.} & \textbf{\% Accuracy} & \textbf{F-Measure} & \textbf{ROC area} \\
K2 & 89.9083 & 0.900 & 0.977 \\
LAGDHillClimber & 90.8257 & 0.909 & 0.982 \\
SimulatedAnnealing & 94.4954  & 0,945 & 0.986 \\
TAN & 95.4128 & 0.954 & 0.992 \\
\end{tabular}
\captionof{table}{Results for default settings and different \# of trees}
\end{center}



 
\subsubsection{Other parameters}

We also tried out other estimator functions, but it turned out that except the “MultiNominalBMAEstimator” function all lead to the same result. “MultiNominalBMAEstimator” just decreases the prediction rate dramatically, which is obvious because the set contains binary nominal data, and not “multinominal” data. After that we also tested the “useADTree” parameter, but it also had no impact on the result. 

\subsection{Conclusion}

In general, all classifiers worked relatively well on this data set and had a prediction rate over 90\%, most of them even over 95\%. It seems, that except of some outliers, the data of the set is well distributed. A sign for this for example is the outcome of the “kNN” tests. There we have seen that almost 92\% of all instances have a closest neighbour with the same class. And also with a greater value of “k”, the results do not change dramatically. Also the difference between a unpruned tree and a pruned tree is not that big, which means that even the unpruned tree gives us a good generalisation of the data. 
 
 
 



\section{Conclusion}
TODO\\



\end{document}
